\chapter{QoS metrics}
\label{cha:quality_metrics}

If you go to the postal office to submit a packet, you will be offered different options.
In addition to the regular service, it is possible that an urgent service exists.
Probably there is also the possibility sending the packet as certified mail, and some option for delivery notification.
It is likely that there are also special services for packets that are voluminous or heavy.

QoS-enabled packet switched networks also offer different kinds of services for data packet delivery.
In this chapter, we will review the different metrics that are relevant for data networks.
This metrics can be used to establish \emph{service level agreements} (SLAs) which are contracts specifying the QoS expected from a network.
These contracts should also specify how the metrics are actually measured.

As an example, if a network guarantees a delay below 100 ms, it should be specified whether this makes reference to the maximum, the average delay or the 95\% percentile.
The measures will also differ depending whether 5 minutes averages or 1 hour averages are considered.
This makes the specification of SLAs tricky.

\section{Delay}

Delay is the time that it is required to traverse the network from the entry point to the exit point.
Delay is normally considered for real-time services such as voice over IP (VoIP).
The total end-to-end delay is simply the sum of the delay suffered in each of the hops in the data network.
As an example, \ref{fig:four_hops} shows a network with four hops.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/four_hops.eps}
\caption{A network with two terminals, three routers and four hops.}
\label{fig:four_hops}
\end{figure}

In each hop, there are four different contributions to delay:
\begin{enumerate}
\item Processing: The time required for the router or switching device to put the packet on the outgoing interface queue.
Very short.
\item Queueing: Waiting time on the outgoing interface queue.
Very short if the queue is empty.
\item Transmission: The time required to put the packet on the transmission medium. It is a function of the packet length and transmission rate.
Short in high-speed transmission media.
\item Propagation: The time that it takes for the packet to travel the distance from the hop source to the hop destination.
Short time over short distances. 
And very long when it involves a trip to a geostationary satellite.
\end{enumerate}

\section{Jitter}
Jitter is the variation of delay.
This aspect is specially relevant for real-time and streaming applications.
These applications expect the packets to arrive regularly in time.
As an example, if the encoder application takes a voice stream and splits it into 20 ms chunks that are encoded and sent as packets, the receiver application will expect to receive one packet every 20 ms to reconstruct the voice stream.

If packets are sent every 20 ms but each of them requires a different time to traverse the network, the separation between packets will no longer be 20 ms at the receiving end.
Applications sensitive to jitter use a de-jittering buffer that holds some packets and feeds the decoder at regular intervals.
The packets that suffered a short delay in the network will wait for a longer time in the de-jitter buffer and the packets that suffered a longer delay in the network stay in the buffer for a shorter time.
With this technique, jitter is effectively suppressed at the expense of increasing delay, as shown in figure \ref{fig:dejitter}.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/dejitter.eps}
\caption{The dejitter buffer removes jitter at the expense of adding delay.}
\label{fig:dejitter}
\end{figure}

The buffer size in terms of time and packets (or bytes) need to be carefully tuned, to prevent both packet overflow and underflow.
Buffer overflow happens when the buffer is full and cannot accommodate an arriving packet.
Buffer underflow occurs when the decoder asks for a packet and the buffer is empty.

\section{Round Trip Delay}

The round trip delay is the measure of delay used for elastic and interactive applications.
It is the time required for a packet from source to destination and then back to the source again.
You can easily find the round trip delay from your host using the ping command.

\begin{lstlisting}
$ ping www.happyforecast.com
PING happyforecast.com (184.107.100.65) 56(84) bytes of data.
64 bytes from s106.panelboxmanager.com (184.107.100.65): icmp_req=1 ttl=48 time=122 ms
64 bytes from s106.panelboxmanager.com (184.107.100.65): icmp_req=2 ttl=48 time=121 ms
64 bytes from s106.panelboxmanager.com (184.107.100.65): icmp_req=3 ttl=48 time=122 ms
64 bytes from s106.panelboxmanager.com (184.107.100.65): icmp_req=4 ttl=48 time=122 ms
64 bytes from s106.panelboxmanager.com (184.107.100.65): icmp_req=5 ttl=48 time=122 ms
64 bytes from s106.panelboxmanager.com (184.107.100.65): icmp_req=6 ttl=48 time=121 ms
^C
--- happyforecast.com ping statistics ---
6 packets transmitted, 6 received, 0% packet loss, time 5004ms
rtt min/avg/max/mdev = 121.864/122.066/122.181/0.230 ms
\end{lstlisting}

An interactive application (such as web browsing) requires a few round trip delays to complete the dialogue between the client and the server.
Furthermore, the round trip delay also limits how fast the Transfer Control Protocol (TCP) congestion window can grow.
The congestion window grows upon the reception of TCP acknowledgements.
If it takes long for the TCP acks to arrive, the congestion window grows slowly.

In fact, it becomes difficult to take fully advantage of connections with a high \mbox{bandwidth x delay} product.
These are known as ``long fat pipes'' or ``long fat networks'' (which is shortened as LFN and sometimes pronounced elephant), as explained in RFC 1072 \cite{rfc1072}.

\section{Packet Loss}

The networks may loose some of the packets being sent.
Packet loss has different sources:
\begin{itemize}
\item Physical layer errors. 
The physical layer may flip some of the bits of the packet.
As the packet typically includes some ``cyclic redundancy check'' (CRC), this errors are detected and the whole packet is dropped.
In wireless communications, the occurrence of errors is likely and therefore some additional mechanisms are used.
Forward error correction (FEC) codes can correct some of the errors.
Furthermore, automatic repeat request (ARQ) are used to request retransmission of faulty packets.
\item Queue packet dropping.
In normal network conditions the queue should be almost empty.
However, when the queues start to fill up, it is necessary to discard packets.
Packets can also be discarded preventively, to regulate the pace of TCP flows.
\item Network failure.
Network failure can result in the loss of many packets, depending on the gravity of the failure and the restoration time.
\end{itemize}

From a QoS perspective, it is not only important how many packets are lost, but also the distribution of the loss in the data flow.
As an example consider two networks that loose 10 packets out of 1000. 
The first network looses a packet out of every 100 packets, while the second one looses the 10 packets consecutively. 

A VoIP application using packet loss concealment will be able to hide the packet loss of the first network, but not of the second one.
If a packet voice is lost, the VoIP application can play the trick of playing the previous packet for two consecutive times, and the listener will hardly notice it.
However, the loss of 10 consecutive packets may represent 2 seconds worth of audio that will be certainly noticed by the listener.

For a TCP application, the loss of a single packet will result in a quick retransmission.
However, loosing 10 consecutive packets may move the TCP session back to the ``slow start'' mode and substantially reduce the throughput.

\section{Bandwidth}

Bandwidth or throughput is the amount of data transmitted per unit of time.
Unfortunately, in practice it may have several different meanings.

It can be related to the line speed, which is the maximum data achieved by the physical medium (e.g, 100 Mbps or 1Gbps).
Some technologies have a variable data rate, such as WiFi, that adapts the transmission speed to channel conditions.
Furthermore, for a given technology, the bandwidth depends on which is the protocol layer under consideration.
As an example, a 54 Mbps WiFi device is capable of 54Mbps at the PHY layer, but roughly half of that is available at the network layer.
The protocol overhead decrease the bandwidth as we move up the layer stack.

Quite often the term bandwidth refers to the amount of data per unit of time that it is actually sent, not at the capabilities of the network.
Or it may refer to the amount of bandwidth that has been contracted to the Internet Service Provider (ISP).
It was common to define this contracts in terms of ``committed information rate'' (CIR) that the ISP promised to carry, and the ``peak information rate'' (PIR) that the ISP would carry if it was possible.
This bandwidth could be measured using ``token buckets'' that allow for some burstiness. 

Another interesting concept is the 95th percentile billing, which is the kind of contract between ISPs and heavy traffic.
In this contract there is a commit (or baseline speed) for a given price (say 20 Mbps for 180 Euro).
The bandwidth consumption is measured in 5 minutes averages, and the 95th percentile value is considered.
Then, if the 95th percentile is higher than the committed rate, the customer has to pay extra fee for each Mbps exceeding the committed rate (e.g., 10 Euro per additional Mbps).

\section{Packet re-ordering}

Many applications need that the packets are received in the same order than they are sent.
To this end, TCP re-order packets and don't offer the data to the upper layers if there are gaps in the data flow.
Similarly, in a VoIP application the voice packets should be handled to the decoder in order.

It is a desirable property of the networks that the maintain packet ordering.
Some basic principles is assign packets belonging to the same flow to the same queue.
In case of a router that performs load balancing among different links, a hash of the source/destination ip address/port can be used to decide the path of the packet, to make sure that all packets of the same flow follow the same path.

\section{Availability}

High availability is a critical issue for data networks.
It is often said that ``five nines'' or 99.999\% is required from carrier grade networks.
This means 5.26 minutes of (non-scheduled) downtime per year.

Some common metrics in terms of availability are the mean time between failures (MTBF) and the mean time to restore (MTTR).
The availability can then be computed as $Availabilty = \frac{MTBF}{MTBF+MTTR}$.

Given a system which is composed of several subsystems either in ``series'' or in ``parallel'', it is possible to compute the syste overall availability.
In ``series'' means that the system works only when all subsystems are working, and in ``parallel'' means that the system fails only when all subsystems fail.

\section{Application requirements}
Broadly speaking, we can classify the applications in two different kinds: inelastic (or real-time) and elastic (non-real time).
An example of an inelastic application is VoIP and an example of an elastic one is remote backup.

Inelastic applications have a given bandwidth (and delay) requirements and are not flexible at all about this.
As an example, if an streaming service requires 2Mbps, it will not work with 1.5 Mbits.
The good part of the inelastic applications is that they only consume what they require.
Using the same example streaming application, it will use only 2 Mbps even if there is more available.

On the other side, elastic applications are more flexible about their requirements.
If a backup service has only 1.5 Mbps available instead of 2 Mbps, the backup will take longer to complete that this is not typically a problem.
Another characteristic of elastic applications is that they are often greedy, which means that they will take as much bandwidth as it is available.
In this attempt to reach the limits of bandwidth availability, they create some temporary congestion or queue build up that is detrimental to real time services.

Somewhere in between real time and non real time applications we find interactive applications. 
They don't have the tight bandwidth and delay constraints of real time applications, but still they have to be responsive for the user to enjoy the experience.
Examples of interactive applications are web browsing, Internet messaging.

\subsection{VoIP}
It has tight delay constraints (below 200ms) and does not consume too much bandwidth.
As delay grows, conversation seems more artificial as there is the sensation that the other party is not responding.
The human conversation protocols break down and it can be irritating.
It is also unpleasant if there are noise and artifacts that make it difficult to understand the conversation, and for this reason packet loss has to be very low.

\subsection{Videoconference}
The requirements for voice are the same as for VoIP.
Regarding the video, it requires more bandwidth but the quality is usually not that important.
Users don't get too annoyed if the image freezes for half a second, as long as the voice quality is fine.

\subsection{Streaming}
It requires a lot of bandwidth and the display quality is important.
The advantage is that it is possible for the receiver to have a buffer to partially alleviate jitter or even packet loss if retransmission is possible.

\subsection{Video on demand}
In this case the, the buffer can be larger as the content is pre-recorded and it is not critical to show it in real-time.

\subsection{Web browsing}
In principle, the load placed by web browsing is low and the delay requirements are mild (a few hundreds of milliseconds is not a problem). 
However, as the video on the web gains popularity, the bandwidth requirements are larger and get closer to those of Video on demand.

\subsection{Peer to peer file exchange and remote backup}
This may involve the transmission of massive amounts of data.
The advantage is that they can often be delayed and accommodated in off-peak hours.

\subsection{Gaming}
Real time online gaming requires short delays.
Normally the volume of data exchanged is low.

\section{Service Level Agreements}

A company owning different networks at different locations normally pays an Internet Service Provider (ISP) for connectivity among those networks.
The ISP offers different services that are labelled with marketing names such as gold, silver and bronze.
Each service has different characteristics in terms of QoS metrics.

The company has to take the decision to take one or more of those services according to the applications that are needed.
Typically, for VoIP applications, the highest QoS (gold) is needed.
Other applications such as nightly backup may use other services that allow to transfer high volumes of data at low rates (bronze).

Besides the QoS commitments, it is common to include in the contract a Committed Information Rate (CIR) and Peak Information Rate (PIR).
The ISP provides QoS guarantees for the traffic which is below the CIR, and allows rates up to the PIR with no QoS guarantees.
As an example, traffic over the CIR and below the PIR may be discarded in case of congestion.

To be more specific, the agreement includes not only a rate, but also a depth of a token bucket, which is a measure of the burstiness of the traffic. 
Token buckets will be covered in more detail in the next chapter.

\subsection{95\% percentile billing}

An alternative of the CIR/PIR approach is the 95\% percentile billing.
The client commits to a given data rate.
Then, the actual traffic is measured in averages of 5 minutes.
The measure that is using for billing is the one that occupies the 95\% percentile.
That means that 95\% of the measures are lower that this value and only the 5\% are higher (peaks).

If the measured 95\% percentile is higher than the committed rate, the client has to pay a penalty that is specified in the SLA.
For example, 10 Euro for each Gbps in excess of the committed rate.
